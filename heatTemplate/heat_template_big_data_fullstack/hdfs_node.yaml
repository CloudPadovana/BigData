heat_template_version: 2017-02-24

parameters:
  image_to_use:
    type: string
    label: Image name or ID
    description: Image used for all nodes in the kafka cluster

  flavor_to_use:
    type: string
    label: Flavor name 
    description: Flavor used for all nodes in the kafka cluster

  key_name_user:
    type: string
    label: Public ssh key of one user.
    description: Public ssh key of one user.

  root_pw:
    type: string
    label: Root password
    description: Root password

  avail_zone:
    type: string
    label: Availability Zone
    description: Availability Zone for the cluster
    default: "nova"

  net_host_prefix:
    type: string
    label: Prefix for the hostname of the cluster nodes
    description: Prefix for the hostname of the cluster nodes.
    default: "hdfs-test"

  tenant_net_id:
    type: string
    label: Network ID of the tenant
    description: This parameter has been set with the id of the tenant network

  tenant_subnet_name:
    type: string
    label: Sub network of the tenant
    description: This parameter has been set with the name of the tenant sub network.

  nameserver_list:
    type: string
    label: Name server ip list
    description: Name server ip list

  sec_group_id:
    type: string
    label: Security Group ID
    description: Security Group ID

  node_id:
    type: number
    label: Node ID
    description: Node ID

  fixed_ip_list:
    type: string
    label: Fixed ip list for hdfs nodes
    description: Fixed ip list for hdfs nodes

  hadoop_ver:
    type: string
    label: Hadoop version
    description: Hadoop version
    default: "2.9.1"


resources:

  hdfs_port:
    type: OS::Neutron::Port
    properties:
      name:
        str_replace:
            template: "hdfs-port-$HDFS_ID"
            params:
                $HDFS_ID: { get_param: node_id }
      network_id: { get_param: tenant_net_id }
      fixed_ips:
        - {
            ip_address: { str_split: [ ' ', { get_param: fixed_ip_list }, { get_param: node_id } ] }, 
            subnet: { get_param: tenant_subnet_name }
          }
      security_groups: [ { get_param: sec_group_id }, ]
  
  hdfs_instance:
    type: OS::Nova::Server
    properties:
      name:  
        str_replace:
            template: "hdfs-instance-$HDFS_ID"
            params:
                $HDFS_ID: { get_param: node_id }
      key_name: { get_param: key_name_user }
      image: { get_param: image_to_use } 
      flavor: { get_param: flavor_to_use }
      admin_pass: { get_param: root_pw }
      networks:
        - port: { get_resource: hdfs_port }
      user_data_format: RAW
      user_data:
        str_replace:
          template: |
            #!/bin/bash
            hostnamectl set-hostname <%HOST_PRE%>-0<%HOST_NUMBER%>.pd.infn.it
            cat > /etc/hosts << EOF
            127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4
            ::1       localhost localhost.localdomain localhost6 localhost6.localdomain6
            EOF
            cat > /etc/ssh/shosts.equiv << EOF
            # Created by HEAT
            EOF
            idx=0
            for ip_item in <%IP_LIST%> ; do
              echo "${ip_item} <%HOST_PRE%>-0${idx}.pd.infn.it <%HOST_PRE%>-0${idx}" >> /etc/hosts
              echo "<%HOST_PRE%>-0${idx}.pd.infn.it" >> /etc/ssh/shosts.equiv
              idx=$[$idx+1]
            done
            cat > /etc/resolv.conf << EOF
            search pd.infn.it
            nameserver <%IP_NS_LIST%>
            EOF
            cat >> /etc/ssh/sshd_config << EOF
            # workaround
            HostbasedAuthentication yes
            IgnoreUserKnownHosts yes
            IgnoreRhosts yes
            EOF
            cat > /etc/ssh/ssh_config << EOF
            Host *
                HostbasedAuthentication yes
                EnableSSHKeysign yes
                GSSAPIAuthentication no
                ForwardX11Trusted yes
                SendEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES
                SendEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT
                SendEnv LC_IDENTIFICATION LC_ALL LANGUAGE
                SendEnv XMODIFIERS
            EOF
            yum install -y vim-enhanced tar wget epel-release java-1.8.0-openjdk python2-pip
            cat >> /etc/sysctl.conf << EOF
            net.ipv6.conf.all.disable_ipv6 = 1
            net.ipv6.conf.default.disable_ipv6 = 1
            EOF
            useradd <%BIGDATA_USR%>
            mkdir -p <%BIGDATA_WD%>
            chown -R <%BIGDATA_USR%>.<%BIGDATA_USR%> <%BIGDATA_WD%>
            wget -O /tmp/hadoop-<%HDOOP_V%>.tar.gz http://artifacts.pd.infn.it/packages/SMACT/misc/hadoop-<%HDOOP_V%>.tar.gz
            runuser -s /bin/bash -c "tar -C <%BIGDATA_WD%> -zxf /tmp/hadoop-<%HDOOP_V%>.tar.gz" -- <%BIGDATA_USR%>
            mv <%BIGDATA_WD%>/hadoop-<%HDOOP_V%> <%BIGDATA_WD%>/hadoop
            cat > /etc/profile.d/hadoop.sh << EOF
            export JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk
            export JRE_HOME=/usr/lib/jvm/jre-1.8.0-openjdk/jre
            export HADOOP_PREFIX=<%BIGDATA_WD%>/hadoop
            export HADOOP_HOME=\$HADOOP_PREFIX
            export HADOOP_COMMON_HOME=\$HADOOP_PREFIX
            export HADOOP_CONF_DIR=\$HADOOP_PREFIX/etc/hadoop
            export HADOOP_HDFS_HOME=\$HADOOP_PREFIX
            export HADOOP_MAPRED_HOME=\$HADOOP_PREFIX
            export HADOOP_YARN_HOME=\$HADOOP_PREFIX
            export PYTHONPATH=\$PYTHONPATH:<%BIGDATA_WD%>/spark/python
            export PATH=\$PATH:<%BIGDATA_WD%>/hadoop/sbin:<%BIGDATA_WD%>/hadoop/bin:<%BIGDATA_WD%>/spark/bin:<%BIGDATA_WD%>/spark/sbin
            EOF
            mkdir /home/<%BIGDATA_USR%>/datanode /home/<%BIGDATA_USR%>/namenode
            chown <%BIGDATA_USR%>.<%BIGDATA_USR%> /home/<%BIGDATA_USR%>/datanode /home/<%BIGDATA_USR%>/namenode
            cat > <%BIGDATA_WD%>/hadoop/etc/hadoop/core-site.xml << EOF
            <configuration>
              <property>
                <name>fs.defaultFS</name>
                <value>hdfs://<%HOST_PRE%>-00.pd.infn.it:9000/</value>
              </property>
            </configuration>
            EOF
            cat > <%BIGDATA_WD%>/hadoop/etc/hadoop/hdfs-site.xml << EOF
            <configuration>
              <property>
                <name>dfs.replication</name>
                <value>1</value>
              </property>
              <property>
                <name>dfs.permissions</name>
                <value>false</value>
              </property>
              <property>
                <name>dfs.datanode.data.dir</name>
                <value>/home/<%BIGDATA_USR%>/datanode</value>
              </property>
              <property>
                <name>dfs.namenode.name.dir</name>
                <value>/home/<%BIGDATA_USR%>/namenode</value>
              </property>
              <property>
                <name>dfs.secondary.http.address</name>
                <value><%MASTER_IP%>:50090</value>
              </property>
            </configuration>
            EOF
            cat > <%BIGDATA_WD%>/hadoop/etc/hadoop/mapred-site.xml << EOF
            <configuration>
              <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
              </property>
            </configuration>
            EOF
            cat > <%BIGDATA_WD%>/hadoop/etc/hadoop/yarn-site.xml << EOF
            <configuration>
              <property>
                <name>yarn.resourcemanager.hostname</name>
                <value><%HOST_PRE%>-00</value>
              </property>
              <property>
                <name>yarn.nodemanager.hostname</name>
                <value><%HOST_PRE%>-00</value>
              </property>
              <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
              </property>
            </configuration>
            EOF
            cat > <%BIGDATA_WD%>/hadoop/etc/hadoop/slaves << EOF
            <%HOST_PRE%>-01.pd.infn.it
            <%HOST_PRE%>-02.pd.infn.it
            EOF
            if [ <%HOST_NUMBER%> -eq 0 ]; then
              runuser -s /bin/bash -c "JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk <%BIGDATA_WD%>/hadoop/bin/hdfs namenode -format" -- <%BIGDATA_USR%>
            fi
            wget -O /tmp/spark-2.3.2.tar.gz http://artifacts.pd.infn.it/packages/SMACT/misc/spark-2.3.2-bin-without-hadoop.tgz
            runuser -s /bin/bash -c "tar -C <%BIGDATA_WD%> -zxf /tmp/spark-2.3.2.tar.gz" -- <%BIGDATA_USR%>
            mv <%BIGDATA_WD%>/spark-2.3.2-bin-without-hadoop <%BIGDATA_WD%>/spark
            cat > <%BIGDATA_WD%>/spark/conf/spark-env.sh << EOF
            export SPARK_DIST_CLASSPATH=\$(<%BIGDATA_WD%>/hadoop/bin/hadoop --config <%BIGDATA_WD%>/hadoop/etc/hadoop classpath)
            export SPARK_WORKER_PORT=<%SPARK_WPORT%>
            EOF
            cat > <%BIGDATA_WD%>/spark/conf/slaves << EOF
            <%HOST_PRE%>-01.pd.infn.it
            <%HOST_PRE%>-02.pd.infn.it
            EOF
            yum -y install python2-py4j python2-root python2-numpy python-pandas
          params:
            <%IP_LIST%>:      { get_param: fixed_ip_list }
            <%IP_NS_LIST%>:   { get_param: nameserver_list }
            <%HOST_NUMBER%>:  { get_param: node_id }
            <%HOST_PRE%>:     { get_param: net_host_prefix }
            <%HDOOP_V%>:      { get_param: hadoop_ver }
            <%BIGDATA_WD%>:   "/opt/bigdata"
            <%BIGDATA_USR%>:  "bigdatausr"
            <%SPARK_WPORT%>:  "6066"
            <%MASTER_IP%>:    { str_split: [' ', { get_param: fixed_ip_list }, 0] }

#
# Post-inst commands:
# ssh-keyscan -t rsa1,rsa,dsa -f /etc/ssh/shosts.equiv >> /etc/ssh/ssh_known_hosts
# systemctl restart sshd
# (bigdatausr on master) hdfs namenode -format
# (bigdatausr on master) start-dfs.sh
# (bigdatausr on master) start-master.sh
# (bigdatausr on slave) start-slave.sh spark://hdfs-test-00.pd.infn.it:7077
#
# https://www.linode.com/docs/databases/hadoop/how-to-install-and-set-up-hadoop-cluster/
# https://pandas.pydata.org/
# http://www.numpy.org/
#
# latest package for spark-root in maven central: "org.diana-hep:spark-root_2.11:0.1.16"
# python call:
#   spark.read.format("org.dianahep.sparkroot.experimental").load("hdfs:///path/to/file.root")
#




